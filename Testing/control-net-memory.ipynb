{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"73650ccf2c79435fb2917b4feb8cb7a9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6957f6a2212044f2b366f8bc7c3b54ef","IPY_MODEL_9f4d13ca233a4fb3a7a355fa8688a34d","IPY_MODEL_59b7c5161bb743ff9a718fb145f717e8"],"layout":"IPY_MODEL_9f79b4e45e464e388f5fa7dd796f45c5"}},"6957f6a2212044f2b366f8bc7c3b54ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2575d8bd41614aeab4ba6d81d3e40361","placeholder":"​","style":"IPY_MODEL_1db6ef6cbbb84e17a6ee48648f1449e5","value":"Fetching 15 files: 100%"}},"9f4d13ca233a4fb3a7a355fa8688a34d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8011d748ddbc4b0190c56a3801208a50","max":15,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ce523336c92c42bd8832a8154153499f","value":15}},"59b7c5161bb743ff9a718fb145f717e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e9e4d68017645789ab5da49aee40526","placeholder":"​","style":"IPY_MODEL_659ba722fe084e4a8081b51f595ef477","value":" 15/15 [00:00&lt;00:00, 32.27it/s]"}},"9f79b4e45e464e388f5fa7dd796f45c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2575d8bd41614aeab4ba6d81d3e40361":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1db6ef6cbbb84e17a6ee48648f1449e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8011d748ddbc4b0190c56a3801208a50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce523336c92c42bd8832a8154153499f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8e9e4d68017645789ab5da49aee40526":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"659ba722fe084e4a8081b51f595ef477":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5488e922c0e4b918e5b9f20459a6e06":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b952f2061fa4ea18d39858d8ab05560","IPY_MODEL_38e43f9713dd4987913a030f48657048","IPY_MODEL_4aa25ad106084e108282f78e3ce0d2b6"],"layout":"IPY_MODEL_54e488a1bcf84bd2be116c08f3d13bf0"}},"6b952f2061fa4ea18d39858d8ab05560":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6948a7c030f742bc9c9e0d3a2cff3a98","placeholder":"​","style":"IPY_MODEL_564490509cbf43d69649855c4214c9d0","value":"scheduler_config.json: 100%"}},"38e43f9713dd4987913a030f48657048":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0996c8ddafc94dea9801523d02624846","max":308,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e7734601d6cf48c3a022cf308ac87ee6","value":308}},"4aa25ad106084e108282f78e3ce0d2b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d451403b47c4b20a5473d208bb1d731","placeholder":"​","style":"IPY_MODEL_8e9fd25e0dac42ba9f3dec27aa4b54a0","value":" 308/308 [00:00&lt;00:00, 33.1kB/s]"}},"54e488a1bcf84bd2be116c08f3d13bf0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6948a7c030f742bc9c9e0d3a2cff3a98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"564490509cbf43d69649855c4214c9d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0996c8ddafc94dea9801523d02624846":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7734601d6cf48c3a022cf308ac87ee6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6d451403b47c4b20a5473d208bb1d731":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e9fd25e0dac42ba9f3dec27aa4b54a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a2eaa6e30f1c46998b8b4faf6a7d2d1e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e0b01cf0d4284b11b3d01f3861ffea12","IPY_MODEL_fbc905fdc791414d9a3c37b8ca42b51b","IPY_MODEL_39f58b2e197f4d5084079a4a3df6120d"],"layout":"IPY_MODEL_13a34cf3a9ce410792e886435aae63ed"}},"e0b01cf0d4284b11b3d01f3861ffea12":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1103bfb7f5a44ec9b54168b5e38ffecf","placeholder":"​","style":"IPY_MODEL_661cefe834984391abed4360d6c339ea","value":"Loading pipeline components...: 100%"}},"fbc905fdc791414d9a3c37b8ca42b51b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9840b102c75479da65c191631fa450b","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c776d3bf081742839b334262218d0358","value":7}},"39f58b2e197f4d5084079a4a3df6120d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a2b18dc755e4d238bf12d0e4ac9f327","placeholder":"​","style":"IPY_MODEL_fd40c5fc9bad46068a38fbbef40562b1","value":" 7/7 [00:28&lt;00:00,  6.81s/it]"}},"13a34cf3a9ce410792e886435aae63ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1103bfb7f5a44ec9b54168b5e38ffecf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"661cefe834984391abed4360d6c339ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b9840b102c75479da65c191631fa450b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c776d3bf081742839b334262218d0358":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8a2b18dc755e4d238bf12d0e4ac9f327":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd40c5fc9bad46068a38fbbef40562b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"11ef0bcf93b64dea80f3647eb39cf23d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7de4a1dfaec34fa69bc4b1d0cd985e1f","IPY_MODEL_e3ebb6f768af4817a8c0d403f2e0d047","IPY_MODEL_505eba9fb9b24e91a7fa8bde0da82358"],"layout":"IPY_MODEL_50b25a350f3245bdb71110a5eb5182b0"}},"7de4a1dfaec34fa69bc4b1d0cd985e1f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3731682cc48436fa8581a5bac729760","placeholder":"​","style":"IPY_MODEL_aafa3651a8cc4ffcb40679fc3b4a715d","value":"Loading pipeline components...: 100%"}},"e3ebb6f768af4817a8c0d403f2e0d047":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6ea7ff80db9468c912389b599558405","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5cc31d3a0e89444c88b4a34762bd3b3d","value":7}},"505eba9fb9b24e91a7fa8bde0da82358":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_90953d1f113e453a985135cc8116e924","placeholder":"​","style":"IPY_MODEL_12d0863cebe34429ba34ebea608b15ee","value":" 7/7 [00:02&lt;00:00,  2.53it/s]"}},"50b25a350f3245bdb71110a5eb5182b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3731682cc48436fa8581a5bac729760":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aafa3651a8cc4ffcb40679fc3b4a715d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c6ea7ff80db9468c912389b599558405":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cc31d3a0e89444c88b4a34762bd3b3d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"90953d1f113e453a985135cc8116e924":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12d0863cebe34429ba34ebea608b15ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53f3b47986f2434cbe37a8f37cc6eb11":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6650369f9a9d4fa9800fdf25fa25d6f4","IPY_MODEL_945e6c53a92c4cb087f1226f8519ce47","IPY_MODEL_829e45fa704547aa826ae0b8465cc8e1"],"layout":"IPY_MODEL_d094badc961a4356b2dc93db9ae94e17"}},"6650369f9a9d4fa9800fdf25fa25d6f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e76f45c0c94e49d7ac0c3cad70e94e26","placeholder":"​","style":"IPY_MODEL_83be80af52b94182b6e75dec6f48fdbe","value":"Loading pipeline components...: 100%"}},"945e6c53a92c4cb087f1226f8519ce47":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc5dc650224b43e2a0f55282547dc348","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_da1d83011add420b8645de8c55a70b46","value":7}},"829e45fa704547aa826ae0b8465cc8e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4861f1860354de7a1997e690107e66d","placeholder":"​","style":"IPY_MODEL_a6b367a09287464aadae3a47f037b74d","value":" 7/7 [00:21&lt;00:00,  5.60s/it]"}},"d094badc961a4356b2dc93db9ae94e17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e76f45c0c94e49d7ac0c3cad70e94e26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83be80af52b94182b6e75dec6f48fdbe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc5dc650224b43e2a0f55282547dc348":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da1d83011add420b8645de8c55a70b46":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f4861f1860354de7a1997e690107e66d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6b367a09287464aadae3a47f037b74d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bfbe1e9e7f714d9f800425c19a59cfd9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6975f293622f4984adf877cd97d29289","IPY_MODEL_fbbd87b99481475ba659c8e42a26fba4","IPY_MODEL_8fdeb90ae47c4481981ed908aa3318ce"],"layout":"IPY_MODEL_28a983dc8314407896b0e16f9ddb8619"}},"6975f293622f4984adf877cd97d29289":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6840cdbe39484eb481bb5197c15eb77f","placeholder":"​","style":"IPY_MODEL_9e163df3a1ed48e19bd9ceb91be477fa","value":"  0%"}},"fbbd87b99481475ba659c8e42a26fba4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_410bc1b5ea634a9583e7a0b01afcbd03","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cdf134e8334640d0a262f9f725a49e28","value":0}},"8fdeb90ae47c4481981ed908aa3318ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bcdb0e37d12465783e9da7e669c0e00","placeholder":"​","style":"IPY_MODEL_60c50e4299fb45c0a232a6834f166913","value":" 0/50 [00:00&lt;?, ?it/s]"}},"28a983dc8314407896b0e16f9ddb8619":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6840cdbe39484eb481bb5197c15eb77f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e163df3a1ed48e19bd9ceb91be477fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"410bc1b5ea634a9583e7a0b01afcbd03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdf134e8334640d0a262f9f725a49e28":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9bcdb0e37d12465783e9da7e669c0e00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60c50e4299fb45c0a232a6834f166913":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T3vAGVJXHT6D","executionInfo":{"status":"ok","timestamp":1742594092395,"user_tz":-330,"elapsed":4888,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}},"outputId":"73c0d6ab-28c0-4ff6-8e4a-66eeca808023"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","!pip install diffusers[torch] transformers\n","!pip install controlnet_aux\n","!pip install opencv-python\n","!pip install matplotlib\n","!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZP8yKXUXHXsy","executionInfo":{"status":"ok","timestamp":1742594248742,"user_tz":-330,"elapsed":156352,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}},"outputId":"f040c896-e026-4438-ae21-c3b6b743d30c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://download.pytorch.org/whl/cu118\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n","INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n","Collecting torch\n","  Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (27 kB)\n","Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (848.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.6.0+cu124\n","    Uninstalling torch-2.6.0+cu124:\n","      Successfully uninstalled torch-2.6.0+cu124\n","Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.6.0+cu118\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n","Requirement already satisfied: diffusers[torch] in /usr/local/lib/python3.11/dist-packages (0.32.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers[torch]) (8.6.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers[torch]) (3.18.0)\n","Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from diffusers[torch]) (0.29.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers[torch]) (2.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers[torch]) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers[torch]) (2.32.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers[torch]) (0.5.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers[torch]) (11.1.0)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.11/dist-packages (from diffusers[torch]) (2.6.0+cu118)\n","Requirement already satisfied: accelerate>=0.31.0 in /usr/local/lib/python3.11/dist-packages (from diffusers[torch]) (1.5.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.31.0->diffusers[torch]) (5.9.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.2->diffusers[torch]) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.2->diffusers[torch]) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->diffusers[torch]) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->diffusers[torch]) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->diffusers[torch]) (11.8.89)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->diffusers[torch]) (11.8.89)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->diffusers[torch]) (11.8.87)\n","Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->diffusers[torch]) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->diffusers[torch]) (11.11.3.6)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->diffusers[torch]) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->diffusers[torch]) (10.3.0.86)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->diffusers[torch]) (11.4.1.48)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->diffusers[torch]) (11.7.5.86)\n","Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->diffusers[torch]) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->diffusers[torch]) (11.8.86)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->diffusers[torch]) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->diffusers[torch]) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4->diffusers[torch]) (1.3.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers[torch]) (3.21.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers[torch]) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers[torch]) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers[torch]) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers[torch]) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4->diffusers[torch]) (3.0.2)\n","Collecting controlnet_aux\n","  Downloading controlnet_aux-0.0.9-py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from controlnet_aux) (2.6.0+cu118)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from controlnet_aux) (8.6.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from controlnet_aux) (0.29.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from controlnet_aux) (1.14.1)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from controlnet_aux) (4.11.0.86)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from controlnet_aux) (3.18.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from controlnet_aux) (2.0.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from controlnet_aux) (11.1.0)\n","Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from controlnet_aux) (0.8.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from controlnet_aux) (0.21.0+cu124)\n","Collecting timm<=0.6.7 (from controlnet_aux)\n","  Downloading timm-0.6.7-py3-none-any.whl.metadata (33 kB)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from controlnet_aux) (0.25.2)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->controlnet_aux) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->controlnet_aux) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->controlnet_aux) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->controlnet_aux) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch->controlnet_aux) (11.8.89)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch->controlnet_aux) (11.8.89)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch->controlnet_aux) (11.8.87)\n","Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->controlnet_aux) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch->controlnet_aux) (11.11.3.6)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch->controlnet_aux) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch->controlnet_aux) (10.3.0.86)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch->controlnet_aux) (11.4.1.48)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch->controlnet_aux) (11.7.5.86)\n","Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->controlnet_aux) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch->controlnet_aux) (11.8.86)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->controlnet_aux) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->controlnet_aux) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->controlnet_aux) (1.3.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->controlnet_aux) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->controlnet_aux) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->controlnet_aux) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->controlnet_aux) (4.67.1)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->controlnet_aux) (3.21.0)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->controlnet_aux) (2.37.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->controlnet_aux) (2025.3.13)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->controlnet_aux) (0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->controlnet_aux) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->controlnet_aux) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->controlnet_aux) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->controlnet_aux) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->controlnet_aux) (2025.1.31)\n","Downloading controlnet_aux-0.0.9-py3-none-any.whl (282 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.4/282.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading timm-0.6.7-py3-none-any.whl (509 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: timm, controlnet_aux\n","  Attempting uninstall: timm\n","    Found existing installation: timm 1.0.15\n","    Uninstalling timm-1.0.15:\n","      Successfully uninstalled timm-1.0.15\n","Successfully installed controlnet_aux-0.0.9 timm-0.6.7\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Collecting datasets\n","  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.0\n","    Uninstalling fsspec-2025.3.0:\n","      Successfully uninstalled fsspec-2025.3.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.4.1 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}]},{"cell_type":"code","source":["!pip install -U diffusers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fZTpYMAwQJjQ","executionInfo":{"status":"ok","timestamp":1742594251861,"user_tz":-330,"elapsed":3117,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}},"outputId":"7d6a389f-b41a-4249-faf6-b4fbe04ac25d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.32.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.6.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers) (3.18.0)\n","Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.29.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.5.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers) (11.1.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.2->diffusers) (2024.12.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.2->diffusers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2025.1.31)\n"]}]},{"cell_type":"code","source":["!pip install -U diffusers transformers torch torchvision accelerate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XBypj7FwQKgs","executionInfo":{"status":"ok","timestamp":1742594273644,"user_tz":-330,"elapsed":21784,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}},"outputId":"367a7268-fba2-4961-e1af-669fd1af8ad7"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.32.2)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n","Collecting transformers\n","  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.6.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers) (3.18.0)\n","Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.29.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.5.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers) (11.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n","Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n","Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2025.1.31)\n","Downloading transformers-4.50.0-py3-none-any.whl (10.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.49.0\n","    Uninstalling transformers-4.49.0:\n","      Successfully uninstalled transformers-4.49.0\n","Successfully installed transformers-4.50.0\n"]}]},{"cell_type":"code","source":["!pip uninstall torch torchvision torchaudio -y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zAe2j1GHQTbx","executionInfo":{"status":"ok","timestamp":1742594276612,"user_tz":-330,"elapsed":2970,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}},"outputId":"8bffffca-e152-40d4-ce9d-efb3e0e2e7b9"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch 2.6.0+cu118\n","Uninstalling torch-2.6.0+cu118:\n","  Successfully uninstalled torch-2.6.0+cu118\n","Found existing installation: torchvision 0.21.0+cu124\n","Uninstalling torchvision-0.21.0+cu124:\n","  Successfully uninstalled torchvision-0.21.0+cu124\n","Found existing installation: torchaudio 2.6.0+cu124\n","Uninstalling torchaudio-2.6.0+cu124:\n","  Successfully uninstalled torchaudio-2.6.0+cu124\n"]}]},{"cell_type":"code","source":["!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KzWm4c6kQWxH","executionInfo":{"status":"ok","timestamp":1742594320832,"user_tz":-330,"elapsed":44215,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}},"outputId":"2820bf7f-d131-40a2-b8cb-61d01b5987e5"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://download.pytorch.org/whl/cu118\n","Collecting torch\n","  Using cached https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (27 kB)\n","Collecting torchvision\n","  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\n","Collecting torchaudio\n","  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n","Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n","Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Using cached https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (848.7 MB)\n","Downloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch, torchvision, torchaudio\n","Successfully installed torch-2.6.0+cu118 torchaudio-2.6.0+cu118 torchvision-0.21.0+cu118\n"]}]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from PIL import Image\n","import os\n","\n","# Function to extract edges from an image\n","def extract_edges(image_path):\n","    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","    edges = cv2.Canny(image, 100, 200)\n","    return edges\n","\n","# Define dataset and output paths\n","dataset_path = \"/content/drive/MyDrive/datafloor\"  # Path to your dataset\n","output_path = \"/content/drive/MyDrive/processed_dataset\"  # Path to save processed images\n","\n","# Create the output folder if it doesn't exist\n","os.makedirs(output_path, exist_ok=True)\n","\n","# Preprocess all images in your dataset\n","for label_folder in os.listdir(dataset_path):\n","    label_path = os.path.join(dataset_path, label_folder)\n","    if os.path.isdir(label_path):\n","        # Create a subfolder in the output directory for each label\n","        output_label_path = os.path.join(output_path, label_folder)\n","        os.makedirs(output_label_path, exist_ok=True)\n","\n","        # Process each image in the label folder\n","        for image_name in os.listdir(label_path):\n","            image_path = os.path.join(label_path, image_name)\n","            edges = extract_edges(image_path)\n","            # Save the processed image\n","            output_image_path = os.path.join(output_label_path, image_name)\n","            cv2.imwrite(output_image_path, edges)\n","\n","print(\"Dataset preprocessing complete!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"85enIt5CHlJH","executionInfo":{"status":"ok","timestamp":1742594350226,"user_tz":-330,"elapsed":29393,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}},"outputId":"934e6c49-bc79-4393-f007-a5aa072f7061"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset preprocessing complete!\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import os\n","\n","class FloorPlanDataset(Dataset):\n","    def __init__(self, image_folder, transform=None):\n","        self.image_folder = image_folder\n","        self.transform = transform\n","        self.image_paths = []\n","        self.labels = []\n","\n","        # Debug: Print the image folder being used\n","        print(f\"Loading dataset from: {image_folder}\")\n","\n","        for label_folder in os.listdir(image_folder):\n","            label_path = os.path.join(image_folder, label_folder)\n","            if os.path.isdir(label_path):\n","                # Debug: Print the label folder being processed\n","                print(f\"Processing label folder: {label_folder}\")\n","                for image_name in os.listdir(label_path):\n","                    image_path = os.path.join(label_path, image_name)\n","                    self.image_paths.append(image_path)\n","                    self.labels.append(int(label_folder.split(\"_\")[0]))  # Extract label from folder name\n","\n","        # Debug: Print the total number of images loaded\n","        print(f\"Total images loaded: {len(self.image_paths)}\")\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx]\n","        label = self.labels[idx]\n","        image = Image.open(image_path).convert(\"RGB\")\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label"],"metadata":{"id":"ZInInXJCIXRK","executionInfo":{"status":"ok","timestamp":1742594354107,"user_tz":-330,"elapsed":3916,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2qVZJCg2JNwt","executionInfo":{"status":"ok","timestamp":1742594354133,"user_tz":-330,"elapsed":24,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UI6CFKoMJUs-","executionInfo":{"status":"ok","timestamp":1742594354141,"user_tz":-330,"elapsed":2,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import os\n","import re  # Regular expressions for extracting numbers\n","\n","class FloorPlanDataset(Dataset):\n","    def __init__(self, image_folder, transform=None):\n","        self.image_folder = image_folder\n","        self.transform = transform\n","        self.image_paths = []\n","        self.labels = []\n","\n","        # Debug: Print the image folder being used\n","        print(f\"Loading dataset from: {image_folder}\")\n","\n","        for label_folder in os.listdir(image_folder):\n","            label_path = os.path.join(image_folder, label_folder)\n","            if os.path.isdir(label_path):\n","                # Debug: Print the label folder being processed\n","                print(f\"Processing label folder: {label_folder}\")\n","\n","                # Extract the numeric part from the folder name (e.g., \"3 rooms\" -> 3)\n","                match = re.search(r'\\d+', label_folder)  # Find the first number in the folder name\n","                if match:\n","                    label = int(match.group())  # Convert the numeric part to an integer\n","                else:\n","                    raise ValueError(f\"Folder name '{label_folder}' does not contain a valid number.\")\n","\n","                # Process each image in the label folder\n","                for image_name in os.listdir(label_path):\n","                    image_path = os.path.join(label_path, image_name)\n","                    self.image_paths.append(image_path)\n","                    self.labels.append(label)  # Use the extracted label\n","\n","        # Debug: Print the total number of images loaded\n","        print(f\"Total images loaded: {len(self.image_paths)}\")\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx]\n","        label = self.labels[idx]\n","        image = Image.open(image_path).convert(\"RGB\")\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label"],"metadata":{"id":"co9xFwXmKhVM","executionInfo":{"status":"ok","timestamp":1742594354178,"user_tz":-330,"elapsed":36,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# Path to the original dataset\n","original_dataset_path = \"/content/drive/MyDrive/datafloor\"\n","\n","# List all folders in the original dataset\n","folders = [f for f in os.listdir(original_dataset_path) if os.path.isdir(os.path.join(original_dataset_path, f))]\n","print(\"Folders in original dataset:\", folders)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VKjKwcHfIyCi","executionInfo":{"status":"ok","timestamp":1742594354183,"user_tz":-330,"elapsed":4,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}},"outputId":"a60cdeec-2cbd-4f05-9ebc-3a529d20caa2"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Folders in original dataset: ['4 rooms', '3 rooms', '5 rooms']\n"]}]},{"cell_type":"code","source":["# Path to the processed dataset\n","processed_dataset_path = \"/content/drive/MyDrive/processed_dataset\"\n","\n","# List all folders in the processed dataset\n","folders = [f for f in os.listdir(processed_dataset_path) if os.path.isdir(os.path.join(processed_dataset_path, f))]\n","print(\"Folders in processed dataset:\", folders)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lHm35e-lLLXd","executionInfo":{"status":"ok","timestamp":1742594354189,"user_tz":-330,"elapsed":5,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}},"outputId":"44844909-e062-49aa-d802-3b19ae69ec64"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Folders in processed dataset: ['4 rooms', '3 rooms', '5 rooms']\n"]}]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from PIL import Image\n","import os\n","\n","# Function to extract edges from an image\n","def extract_edges(image_path):\n","    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","    edges = cv2.Canny(image, 100, 200)\n","    return edges\n","\n","# Define dataset and output paths\n","dataset_path = \"/content/drive/MyDrive/datafloor\"  # Path to your original dataset\n","output_path = \"/content/drive/MyDrive/processed_dataset\"  # Path to save processed images\n","\n","# Create the output folder if it doesn't exist\n","os.makedirs(output_path, exist_ok=True)\n","\n","# Preprocess all images in your dataset\n","for label_folder in os.listdir(dataset_path):\n","    label_path = os.path.join(dataset_path, label_folder)\n","    if os.path.isdir(label_path):\n","        # Create a subfolder in the output directory for each label\n","        output_label_path = os.path.join(output_path, label_folder)\n","        os.makedirs(output_label_path, exist_ok=True)\n","\n","        # Process each image in the label folder\n","        for image_name in os.listdir(label_path):\n","            image_path = os.path.join(label_path, image_name)\n","            edges = extract_edges(image_path)\n","            # Save the processed image\n","            output_image_path = os.path.join(output_label_path, image_name)\n","            cv2.imwrite(output_image_path, edges)\n","\n","print(\"Dataset preprocessing complete!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tA2tuFaOLOuM","executionInfo":{"status":"ok","timestamp":1742594375842,"user_tz":-330,"elapsed":21653,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}},"outputId":"d1b9057b-ecc9-46b0-9abb-d378e9a0ea37"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset preprocessing complete!\n"]}]},{"cell_type":"code","source":["from torchvision import transforms\n","\n","# Define transforms\n","transform = transforms.Compose([\n","    transforms.Resize((512, 512)),\n","    transforms.ToTensor(),\n","])\n","\n","# Create dataset\n","dataset = FloorPlanDataset(image_folder=\"/content/drive/MyDrive/processed_dataset\", transform=transform)\n","\n","# Create dataloader\n","dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n","\n","# Debug: Print a batch of data\n","for batch in dataloader:\n","    images, labels = batch\n","    print(\"Batch images shape:\", images.shape)\n","    print(\"Batch labels:\", labels)\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PtqES_nVLWSc","executionInfo":{"status":"ok","timestamp":1742594383259,"user_tz":-330,"elapsed":7420,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}},"outputId":"831521e2-8215-4dcf-a0d2-45a93215d019"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading dataset from: /content/drive/MyDrive/processed_dataset\n","Processing label folder: 4 rooms\n","Processing label folder: 3 rooms\n","Processing label folder: 5 rooms\n","Total images loaded: 510\n","Batch images shape: torch.Size([4, 3, 512, 512])\n","Batch labels: tensor([5, 3, 4, 4])\n"]}]},{"cell_type":"code","source":["from diffusers import ControlNetModel, StableDiffusionControlNetPipeline\n","import torch\n","\n","# Load ControlNet model\n","controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\")\n","\n","# Load Stable Diffusion model with ControlNet\n","pipe = StableDiffusionControlNetPipeline.from_pretrained(\n","    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\n",").to(\"cuda\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["73650ccf2c79435fb2917b4feb8cb7a9","6957f6a2212044f2b366f8bc7c3b54ef","9f4d13ca233a4fb3a7a355fa8688a34d","59b7c5161bb743ff9a718fb145f717e8","9f79b4e45e464e388f5fa7dd796f45c5","2575d8bd41614aeab4ba6d81d3e40361","1db6ef6cbbb84e17a6ee48648f1449e5","8011d748ddbc4b0190c56a3801208a50","ce523336c92c42bd8832a8154153499f","8e9e4d68017645789ab5da49aee40526","659ba722fe084e4a8081b51f595ef477","a5488e922c0e4b918e5b9f20459a6e06","6b952f2061fa4ea18d39858d8ab05560","38e43f9713dd4987913a030f48657048","4aa25ad106084e108282f78e3ce0d2b6","54e488a1bcf84bd2be116c08f3d13bf0","6948a7c030f742bc9c9e0d3a2cff3a98","564490509cbf43d69649855c4214c9d0","0996c8ddafc94dea9801523d02624846","e7734601d6cf48c3a022cf308ac87ee6","6d451403b47c4b20a5473d208bb1d731","8e9fd25e0dac42ba9f3dec27aa4b54a0","a2eaa6e30f1c46998b8b4faf6a7d2d1e","e0b01cf0d4284b11b3d01f3861ffea12","fbc905fdc791414d9a3c37b8ca42b51b","39f58b2e197f4d5084079a4a3df6120d","13a34cf3a9ce410792e886435aae63ed","1103bfb7f5a44ec9b54168b5e38ffecf","661cefe834984391abed4360d6c339ea","b9840b102c75479da65c191631fa450b","c776d3bf081742839b334262218d0358","8a2b18dc755e4d238bf12d0e4ac9f327","fd40c5fc9bad46068a38fbbef40562b1"]},"id":"1zzZZekaMrYz","executionInfo":{"status":"ok","timestamp":1742594513414,"user_tz":-330,"elapsed":38674,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}},"outputId":"5b28804f-10b7-4e9a-a533-bef770bc8926"},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":["Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73650ccf2c79435fb2917b4feb8cb7a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["scheduler_config.json:   0%|          | 0.00/308 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5488e922c0e4b918e5b9f20459a6e06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2eaa6e30f1c46998b8b4faf6a7d2d1e"}},"metadata":{}}]},{"cell_type":"code","source":["from torch.optim import AdamW\n","from tqdm import tqdm\n","from transformers import CLIPTextModel, CLIPTokenizer\n","import torch\n","import torch.nn.functional as F\n","\n","# Load the tokenizer and text encoder for Stable Diffusion\n","tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n","text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n","\n","# Define optimizer\n","optimizer = AdamW(controlnet.parameters(), lr=1e-5)\n","\n","# Training loop\n","num_epochs = 3\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","    controlnet.train()\n","    epoch_loss = 0\n","\n","    for batch in tqdm(dataloader):\n","        control_images, labels = batch\n","        control_images = control_images.to(\"cuda\")\n","\n","        # Zero gradients\n","        optimizer.zero_grad()\n","\n","        # Prepare text embeddings with proper padding\n","        text_inputs = tokenizer(\n","            [f\"{label}-room floor plan\" for label in labels],\n","            padding=\"max_length\",\n","            max_length=tokenizer.model_max_length,\n","            truncation=True,\n","            return_tensors=\"pt\"\n","        )\n","        text_input_ids = text_inputs.input_ids.to(\"cuda\")\n","\n","        # Get text embeddings\n","        with torch.no_grad():\n","            text_embeddings = text_encoder(text_input_ids)[0]\n","\n","        # Create appropriate noise sample - make sure dimensions match UNet expectations\n","        # Check the expected channel dimension of your UNet\n","        batch_size = control_images.shape[0]\n","\n","        # Most SD models expect 4 channels for latents\n","        # The spatial dimensions should be 1/8 of the original image size\n","        # For 512×512 images, latents would be 64×64\n","        latent_height, latent_width = 64, 64\n","        noise = torch.randn(\n","            (batch_size, 4, latent_height, latent_width),\n","            device=\"cuda\"\n","        )\n","\n","        # Generate random timesteps\n","        timesteps = torch.randint(\n","            0, 1000, (batch_size,),\n","            device=\"cuda\"\n","        ).long()\n","\n","        # Make sure control images match expected input shape\n","        # If control_images aren't already transformed to match ControlNet's expected input\n","        # Add a transformation here if needed\n","\n","        # Forward pass through ControlNet\n","        try:\n","            down_block_res_samples, mid_block_res_sample = controlnet(\n","                sample=noise,\n","                timestep=timesteps,\n","                encoder_hidden_states=text_embeddings,\n","                controlnet_cond=control_images,\n","                return_dict=False,\n","            )\n","\n","            # Simple loss example (regularize outputs)\n","            loss = 0\n","            for res_sample in down_block_res_samples:\n","                loss += res_sample.pow(2).mean()\n","            loss += mid_block_res_sample.pow(2).mean()\n","\n","            # Backward pass\n","            loss.backward()\n","            optimizer.step()\n","\n","            epoch_loss += loss.item()\n","\n","        except Exception as e:\n","            print(f\"Error in batch: {e}\")\n","            print(f\"Text embeddings shape: {text_embeddings.shape}\")\n","            print(f\"Control images shape: {control_images.shape}\")\n","            print(f\"Noise shape: {noise.shape}\")\n","            # Skip problematic batch\n","            continue\n","\n","    avg_epoch_loss = epoch_loss / len(dataloader)\n","    print(f\"Epoch {epoch + 1} Loss: {avg_epoch_loss}\")\n","\n","# Save the fine-tuned model\n","controlnet.save_pretrained(\"/content/drive/MyDrive/fine_tuned_controlnet\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lLDxkEbqx1ax","executionInfo":{"status":"ok","timestamp":1742595626249,"user_tz":-330,"elapsed":58342,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}},"outputId":"525ce720-6bc9-40bb-fc93-ba630e2bd6ae"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n"]},{"output_type":"stream","name":"stderr","text":["  1%|          | 1/128 [00:00<00:16,  7.85it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 2/128 [00:00<00:16,  7.57it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 3/128 [00:00<00:16,  7.81it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 4/128 [00:00<00:15,  8.17it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 5/128 [00:00<00:15,  7.96it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▍         | 6/128 [00:00<00:15,  7.99it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▌         | 7/128 [00:00<00:14,  8.37it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▋         | 8/128 [00:00<00:14,  8.54it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 10/128 [00:01<00:13,  8.70it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▊         | 11/128 [00:01<00:14,  8.34it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▉         | 12/128 [00:01<00:14,  7.98it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 13/128 [00:01<00:13,  8.26it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█         | 14/128 [00:01<00:14,  8.13it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 15/128 [00:01<00:13,  8.19it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▎        | 16/128 [00:01<00:13,  8.36it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 17/128 [00:02<00:12,  8.73it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 18/128 [00:02<00:12,  8.95it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▍        | 19/128 [00:02<00:12,  8.96it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 20/128 [00:02<00:12,  8.88it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▋        | 21/128 [00:02<00:12,  8.65it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 22/128 [00:02<00:13,  8.03it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 23/128 [00:02<00:14,  7.37it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 19%|█▉        | 24/128 [00:02<00:13,  7.73it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|█▉        | 25/128 [00:03<00:13,  7.42it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 26/128 [00:03<00:13,  7.43it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██        | 27/128 [00:03<00:13,  7.52it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 28/128 [00:03<00:12,  7.94it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 23%|██▎       | 29/128 [00:03<00:12,  7.89it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 23%|██▎       | 30/128 [00:03<00:12,  7.61it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 31/128 [00:03<00:12,  7.61it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 25%|██▌       | 32/128 [00:03<00:11,  8.02it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 33/128 [00:04<00:11,  8.45it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 27%|██▋       | 34/128 [00:04<00:11,  8.44it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 27%|██▋       | 35/128 [00:04<00:11,  8.11it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 36/128 [00:04<00:11,  7.82it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▉       | 37/128 [00:04<00:11,  8.07it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|██▉       | 38/128 [00:04<00:12,  7.30it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 39/128 [00:04<00:11,  7.61it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 31%|███▏      | 40/128 [00:04<00:11,  7.83it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 41/128 [00:05<00:10,  8.26it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 42/128 [00:05<00:11,  7.72it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▎      | 43/128 [00:05<00:10,  7.81it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 44/128 [00:05<00:10,  7.87it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 46/128 [00:05<00:09,  8.80it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 38%|███▊      | 48/128 [00:05<00:10,  7.31it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 39%|███▉      | 50/128 [00:06<00:12,  6.41it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 41%|████      | 52/128 [00:06<00:13,  5.61it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 41%|████▏     | 53/128 [00:06<00:13,  5.41it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 43%|████▎     | 55/128 [00:07<00:13,  5.51it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 45%|████▍     | 57/128 [00:07<00:13,  5.45it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 46%|████▌     | 59/128 [00:08<00:13,  5.22it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 48%|████▊     | 61/128 [00:08<00:13,  4.95it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 49%|████▉     | 63/128 [00:08<00:12,  5.24it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 51%|█████     | 65/128 [00:09<00:09,  6.46it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 52%|█████▏    | 67/128 [00:09<00:08,  7.21it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 54%|█████▍    | 69/128 [00:09<00:07,  7.77it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 55%|█████▌    | 71/128 [00:09<00:06,  8.31it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 57%|█████▋    | 73/128 [00:10<00:06,  8.28it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 59%|█████▊    | 75/128 [00:10<00:06,  8.44it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|██████    | 77/128 [00:10<00:06,  8.34it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 61%|██████    | 78/128 [00:10<00:05,  8.76it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 63%|██████▎   | 81/128 [00:11<00:05,  8.57it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 65%|██████▍   | 83/128 [00:11<00:05,  8.09it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▌   | 84/128 [00:11<00:05,  8.03it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 68%|██████▊   | 87/128 [00:11<00:04,  8.65it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 70%|██████▉   | 89/128 [00:11<00:04,  8.33it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 71%|███████   | 91/128 [00:12<00:04,  8.65it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 73%|███████▎  | 93/128 [00:12<00:04,  8.40it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 74%|███████▍  | 95/128 [00:12<00:03,  8.83it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 76%|███████▌  | 97/128 [00:12<00:03,  8.30it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 77%|███████▋  | 99/128 [00:13<00:03,  8.05it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 79%|███████▉  | 101/128 [00:13<00:03,  7.63it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|████████  | 103/128 [00:13<00:03,  7.82it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 82%|████████▏ | 105/128 [00:13<00:03,  7.65it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 84%|████████▎ | 107/128 [00:14<00:02,  8.32it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 85%|████████▌ | 109/128 [00:14<00:02,  8.28it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 87%|████████▋ | 111/128 [00:14<00:02,  7.74it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 88%|████████▊ | 113/128 [00:14<00:01,  7.60it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 90%|████████▉ | 115/128 [00:15<00:01,  8.52it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 91%|█████████▏| 117/128 [00:15<00:01,  8.35it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 93%|█████████▎| 119/128 [00:15<00:01,  8.66it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 95%|█████████▍| 121/128 [00:15<00:00,  8.52it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 96%|█████████▌| 123/128 [00:16<00:00,  8.76it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 98%|█████████▊| 125/128 [00:16<00:00,  9.14it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 128/128 [00:16<00:00,  7.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (154x512 and 768x320)\n","Text embeddings shape: torch.Size([2, 77, 512])\n","Control images shape: torch.Size([2, 3, 512, 512])\n","Noise shape: torch.Size([2, 4, 64, 64])\n","Epoch 1 Loss: 0.0\n","Epoch 2/3\n"]},{"output_type":"stream","name":"stderr","text":["  2%|▏         | 2/128 [00:00<00:15,  8.34it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["  3%|▎         | 4/128 [00:00<00:14,  8.55it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["  5%|▍         | 6/128 [00:00<00:13,  8.95it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▋         | 8/128 [00:00<00:13,  9.20it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["  9%|▊         | 11/128 [00:01<00:13,  8.49it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 10%|█         | 13/128 [00:01<00:13,  8.40it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 12%|█▏        | 15/128 [00:01<00:13,  8.67it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 13%|█▎        | 17/128 [00:01<00:12,  9.17it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 15%|█▍        | 19/128 [00:02<00:12,  8.97it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 16%|█▋        | 21/128 [00:02<00:14,  7.22it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 18%|█▊        | 23/128 [00:02<00:15,  6.59it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|█▉        | 25/128 [00:03<00:16,  6.11it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 21%|██        | 27/128 [00:03<00:16,  6.04it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 23%|██▎       | 29/128 [00:03<00:16,  5.95it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 24%|██▍       | 31/128 [00:04<00:16,  5.99it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 26%|██▌       | 33/128 [00:04<00:15,  6.19it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 27%|██▋       | 35/128 [00:04<00:15,  5.89it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 36/128 [00:05<00:15,  5.82it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▉       | 37/128 [00:05<00:17,  5.24it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 31%|███▏      | 40/128 [00:05<00:13,  6.31it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 42/128 [00:05<00:11,  7.67it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 34%|███▍      | 44/128 [00:06<00:11,  7.44it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 36%|███▌      | 46/128 [00:06<00:10,  7.92it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 38%|███▊      | 48/128 [00:06<00:09,  8.01it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 39%|███▉      | 50/128 [00:06<00:09,  7.95it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 41%|████      | 52/128 [00:07<00:09,  7.74it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 41%|████▏     | 53/128 [00:07<00:09,  7.60it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 44%|████▍     | 56/128 [00:07<00:09,  7.85it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 46%|████▌     | 59/128 [00:08<00:07,  8.97it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 48%|████▊     | 61/128 [00:08<00:07,  8.52it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 62/128 [00:08<00:08,  8.05it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 51%|█████     | 65/128 [00:08<00:07,  8.79it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 52%|█████▏    | 67/128 [00:08<00:07,  8.47it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 55%|█████▍    | 70/128 [00:09<00:06,  9.33it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 56%|█████▋    | 72/128 [00:09<00:06,  8.99it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 57%|█████▋    | 73/128 [00:09<00:06,  8.86it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 59%|█████▉    | 76/128 [00:09<00:06,  8.65it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 62%|██████▎   | 80/128 [00:10<00:05,  9.39it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 64%|██████▍   | 82/128 [00:10<00:05,  8.98it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 66%|██████▌   | 84/128 [00:10<00:05,  8.58it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 67%|██████▋   | 86/128 [00:11<00:04,  8.72it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 69%|██████▉   | 88/128 [00:11<00:04,  8.48it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 70%|███████   | 90/128 [00:11<00:04,  8.63it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 72%|███████▏  | 92/128 [00:11<00:04,  8.79it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 73%|███████▎  | 94/128 [00:12<00:03,  8.61it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 75%|███████▌  | 96/128 [00:12<00:04,  7.95it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 77%|███████▋  | 98/128 [00:12<00:03,  8.10it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 78%|███████▊  | 100/128 [00:12<00:03,  7.87it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|███████▉  | 102/128 [00:13<00:03,  8.13it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|████████  | 103/128 [00:13<00:03,  7.65it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 83%|████████▎ | 106/128 [00:13<00:02,  8.60it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 84%|████████▍ | 108/128 [00:13<00:02,  9.02it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 86%|████████▌ | 110/128 [00:13<00:02,  8.54it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 88%|████████▊ | 112/128 [00:14<00:01,  8.52it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 89%|████████▉ | 114/128 [00:14<00:01,  8.27it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 90%|████████▉ | 115/128 [00:14<00:01,  8.10it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 92%|█████████▏| 118/128 [00:14<00:01,  8.60it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 93%|█████████▎| 119/128 [00:15<00:01,  8.68it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 95%|█████████▌| 122/128 [00:15<00:00,  8.63it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 97%|█████████▋| 124/128 [00:15<00:00,  7.51it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 98%|█████████▊| 126/128 [00:15<00:00,  6.91it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 128/128 [00:16<00:00,  7.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (154x512 and 768x320)\n","Text embeddings shape: torch.Size([2, 77, 512])\n","Control images shape: torch.Size([2, 3, 512, 512])\n","Noise shape: torch.Size([2, 4, 64, 64])\n","Epoch 2 Loss: 0.0\n","Epoch 3/3\n"]},{"output_type":"stream","name":"stderr","text":["  2%|▏         | 2/128 [00:00<00:18,  6.95it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["  3%|▎         | 4/128 [00:00<00:19,  6.40it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["  5%|▍         | 6/128 [00:00<00:20,  5.92it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▌         | 7/128 [00:01<00:23,  5.09it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▋         | 8/128 [00:01<00:25,  4.68it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["  8%|▊         | 10/128 [00:01<00:22,  5.19it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["  9%|▉         | 12/128 [00:02<00:23,  5.02it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 13/128 [00:02<00:22,  5.12it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 12%|█▏        | 15/128 [00:02<00:19,  5.75it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▎        | 16/128 [00:02<00:17,  6.24it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 15%|█▍        | 19/128 [00:03<00:14,  7.72it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 16%|█▋        | 21/128 [00:03<00:13,  8.03it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 22/128 [00:03<00:12,  8.44it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|█▉        | 25/128 [00:03<00:11,  8.64it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██        | 27/128 [00:04<00:11,  8.62it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 23%|██▎       | 29/128 [00:04<00:11,  8.51it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 24%|██▍       | 31/128 [00:04<00:11,  8.73it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 26%|██▌       | 33/128 [00:04<00:10,  8.78it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 28%|██▊       | 36/128 [00:05<00:09,  9.35it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 30%|██▉       | 38/128 [00:05<00:10,  8.91it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 31%|███▏      | 40/128 [00:05<00:09,  9.27it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 34%|███▎      | 43/128 [00:05<00:08,  9.60it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 35%|███▌      | 45/128 [00:06<00:09,  8.34it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 37%|███▋      | 47/128 [00:06<00:09,  8.90it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 38%|███▊      | 49/128 [00:06<00:09,  8.74it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 39%|███▉      | 50/128 [00:06<00:08,  8.74it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 41%|████▏     | 53/128 [00:07<00:08,  8.77it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 43%|████▎     | 55/128 [00:07<00:08,  8.46it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 45%|████▍     | 57/128 [00:07<00:08,  8.70it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 45%|████▌     | 58/128 [00:07<00:08,  8.32it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 48%|████▊     | 61/128 [00:07<00:07,  8.70it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 49%|████▉     | 63/128 [00:08<00:07,  8.50it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 51%|█████     | 65/128 [00:08<00:07,  9.00it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 66/128 [00:08<00:07,  8.75it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 54%|█████▍    | 69/128 [00:08<00:06,  8.82it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 55%|█████▌    | 71/128 [00:09<00:06,  8.81it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 57%|█████▋    | 73/128 [00:09<00:06,  8.66it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 59%|█████▊    | 75/128 [00:09<00:06,  8.79it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 61%|██████    | 78/128 [00:09<00:05,  9.25it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 62%|██████▎   | 80/128 [00:10<00:05,  8.76it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 64%|██████▍   | 82/128 [00:10<00:05,  8.74it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 66%|██████▌   | 84/128 [00:10<00:05,  7.98it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 68%|██████▊   | 87/128 [00:10<00:04,  8.73it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 70%|██████▉   | 89/128 [00:11<00:04,  7.95it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 71%|███████   | 91/128 [00:11<00:04,  8.06it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 73%|███████▎  | 93/128 [00:11<00:04,  8.07it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 74%|███████▍  | 95/128 [00:11<00:03,  8.44it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 76%|███████▌  | 97/128 [00:12<00:03,  7.97it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 77%|███████▋  | 99/128 [00:12<00:03,  8.29it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 78%|███████▊  | 100/128 [00:12<00:03,  8.55it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|███████▉  | 102/128 [00:13<00:04,  6.16it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 81%|████████▏ | 104/128 [00:13<00:03,  6.00it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 83%|████████▎ | 106/128 [00:13<00:03,  5.78it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 84%|████████▎ | 107/128 [00:13<00:03,  5.54it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 85%|████████▌ | 109/128 [00:14<00:03,  5.67it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 87%|████████▋ | 111/128 [00:14<00:02,  6.13it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 88%|████████▊ | 113/128 [00:14<00:02,  6.45it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 90%|████████▉ | 115/128 [00:15<00:02,  5.60it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 91%|█████████▏| 117/128 [00:15<00:02,  5.47it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r 92%|█████████▏| 118/128 [00:15<00:01,  5.41it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 95%|█████████▍| 121/128 [00:16<00:01,  6.93it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 96%|█████████▌| 123/128 [00:16<00:00,  7.71it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 98%|█████████▊| 125/128 [00:16<00:00,  8.23it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":[" 99%|█████████▉| 127/128 [00:16<00:00,  8.16it/s]"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n","Error in batch: mat1 and mat2 shapes cannot be multiplied (308x512 and 768x320)\n","Text embeddings shape: torch.Size([4, 77, 512])\n","Control images shape: torch.Size([4, 3, 512, 512])\n","Noise shape: torch.Size([4, 4, 64, 64])\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 128/128 [00:17<00:00,  7.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Error in batch: mat1 and mat2 shapes cannot be multiplied (154x512 and 768x320)\n","Text embeddings shape: torch.Size([2, 77, 512])\n","Control images shape: torch.Size([2, 3, 512, 512])\n","Noise shape: torch.Size([2, 4, 64, 64])\n","Epoch 3 Loss: 0.0\n"]}]},{"cell_type":"code","source":["# Set environment variable for memory management\n","import os\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","\n","# Then proceed with model loading\n","torch.cuda.empty_cache()\n","\n","# Load the fine-tuned ControlNet model in half precision\n","fine_tuned_controlnet = ControlNetModel.from_pretrained(\n","    \"/content/drive/MyDrive/fine_tuned_controlnet\",\n","    torch_dtype=torch.float16\n",").to(\"cuda\")\n","\n","# Remaining code..."],"metadata":{"id":"-GyPv2Ni1aFP","executionInfo":{"status":"error","timestamp":1742596326351,"user_tz":-330,"elapsed":5021,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}},"outputId":"58959f1c-32e3-4c6a-b8b0-e929c06c9ec0","colab":{"base_uri":"https://localhost:8080/","height":425}},"execution_count":26,"outputs":[{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 40081 has 14.74 GiB memory in use. Of the allocated memory 14.49 GiB is allocated by PyTorch, and 125.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-3aeac434874d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m\"/content/drive/MyDrive/fine_tuned_controlnet\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m ).to(\"cuda\")\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Remaining code...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/models/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1075\u001b[0m                     \u001b[0;34mf\"The current device is `{self.device}`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m                 )\n\u001b[0;32m-> 1077\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m     \u001b[0;31m# Taken from `transformers`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 40081 has 14.74 GiB memory in use. Of the allocated memory 14.49 GiB is allocated by PyTorch, and 125.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}]},{"cell_type":"code","source":["# Load the fine-tuned ControlNet model in half precision\n","fine_tuned_controlnet = ControlNetModel.from_pretrained(\n","    \"/content/drive/MyDrive/fine_tuned_controlnet\",\n","    torch_dtype=torch.float16\n",").to(\"cuda\")\n","\n","# Load Stable Diffusion with the fine-tuned ControlNet (also in half precision)\n","fine_tuned_pipe = StableDiffusionControlNetPipeline.from_pretrained(\n","    \"runwayml/stable-diffusion-v1-5\",\n","    controlnet=fine_tuned_controlnet,\n","    torch_dtype=torch.float16\n",").to(\"cuda\")\n","\n","# Enable memory efficient attention\n","fine_tuned_pipe.enable_attention_slicing()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"id":"mPtHmd_u1A21","executionInfo":{"status":"error","timestamp":1742596224974,"user_tz":-330,"elapsed":5236,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}},"outputId":"2e4e9e97-33c5-48ee-b3e7-7c6512ccd95b"},"execution_count":25,"outputs":[{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 40081 has 14.73 GiB memory in use. Of the allocated memory 14.47 GiB is allocated by PyTorch, and 140.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-d372c6f513ad>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"/content/drive/MyDrive/fine_tuned_controlnet\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m ).to(\"cuda\")\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load Stable Diffusion with the fine-tuned ControlNet (also in half precision)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/models/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1075\u001b[0m                     \u001b[0;34mf\"The current device is `{self.device}`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m                 )\n\u001b[0;32m-> 1077\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m     \u001b[0;31m# Taken from `transformers`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 40081 has 14.73 GiB memory in use. Of the allocated memory 14.47 GiB is allocated by PyTorch, and 140.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}]},{"cell_type":"code","source":["# First clear cache\n","torch.cuda.empty_cache()\n","\n","# Load the text encoder first\n","text_encoder = CLIPTextModel.from_pretrained(\n","    \"runwayml/stable-diffusion-v1-5\",\n","    subfolder=\"text_encoder\",\n","    torch_dtype=torch.float16\n",").to(\"cuda\")\n","\n","# Load the tokenizer (doesn't use GPU memory)\n","tokenizer = CLIPTokenizer.from_pretrained(\n","    \"runwayml/stable-diffusion-v1-5\",\n","    subfolder=\"tokenizer\"\n",")\n","\n","# Load VAE\n","vae = AutoencoderKL.from_pretrained(\n","    \"runwayml/stable-diffusion-v1-5\",\n","    subfolder=\"vae\",\n","    torch_dtype=torch.float16\n",").to(\"cuda\")\n","\n","# Load the control net model\n","fine_tuned_controlnet = ControlNetModel.from_pretrained(\n","    \"/content/drive/MyDrive/fine_tuned_controlnet\",\n","    torch_dtype=torch.float16\n",").to(\"cuda\")\n","\n","# Load the UNet\n","unet = UNet2DConditionModel.from_pretrained(\n","    \"runwayml/stable-diffusion-v1-5\",\n","    subfolder=\"unet\",\n","    torch_dtype=torch.float16\n",").to(\"cuda\")\n","\n","# Create the pipeline with all components\n","fine_tuned_pipe = StableDiffusionControlNetPipeline(\n","    vae=vae,\n","    text_encoder=text_encoder,\n","    tokenizer=tokenizer,\n","    unet=unet,\n","    controlnet=fine_tuned_controlnet,\n","    scheduler=DDIMScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\"),\n","    safety_checker=None,\n","    feature_extractor=None,\n","    requires_safety_checker=False\n",")\n","\n","# Memory optimizations\n","fine_tuned_pipe.enable_attention_slicing(slice_size=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"id":"Egxc7vzX03Mb","executionInfo":{"status":"error","timestamp":1742596185639,"user_tz":-330,"elapsed":2866,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}},"outputId":"b9d9b836-62b1-48b7-dc1d-a807af3d3c06"},"execution_count":24,"outputs":[{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 40081 has 14.69 GiB memory in use. Of the allocated memory 14.39 GiB is allocated by PyTorch, and 179.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-42e5dfc0bc1f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text_encoder\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m ).to(\"cuda\")\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load the tokenizer (doesn't use GPU memory)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3710\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3711\u001b[0m                 )\n\u001b[0;32m-> 3712\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 40081 has 14.69 GiB memory in use. Of the allocated memory 14.39 GiB is allocated by PyTorch, and 179.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}]},{"cell_type":"code","source":["# Load the fine-tuned ControlNet model in float32 (default)\n","fine_tuned_controlnet = ControlNetModel.from_pretrained(\n","    \"/content/drive/MyDrive/fine_tuned_controlnet\"\n",").to(\"cuda\")\n","\n","# Load Stable Diffusion with the fine-tuned ControlNet in float32\n","fine_tuned_pipe = StableDiffusionControlNetPipeline.from_pretrained(\n","    \"runwayml/stable-diffusion-v1-5\",\n","    controlnet=fine_tuned_controlnet,\n","    # Remove torch_dtype parameter to use float32\n",").to(\"cuda\")\n","\n","# Generate a floor plan\n","prompt = \"a modern 3-room floor plan\"\n","control_image = Image.open(\"/content/drive/MyDrive/processed_dataset/3 rooms/Cat10_1.jpg\")\n","control_image = control_image.resize((512, 512))\n","output_image = fine_tuned_pipe(prompt, control_image, num_inference_steps=50).images[0]\n","\n","# Display the generated image\n","output_image.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":457,"referenced_widgets":["11ef0bcf93b64dea80f3647eb39cf23d","7de4a1dfaec34fa69bc4b1d0cd985e1f","e3ebb6f768af4817a8c0d403f2e0d047","505eba9fb9b24e91a7fa8bde0da82358","50b25a350f3245bdb71110a5eb5182b0","a3731682cc48436fa8581a5bac729760","aafa3651a8cc4ffcb40679fc3b4a715d","c6ea7ff80db9468c912389b599558405","5cc31d3a0e89444c88b4a34762bd3b3d","90953d1f113e453a985135cc8116e924","12d0863cebe34429ba34ebea608b15ee"]},"id":"BbYLd6X30YMD","executionInfo":{"status":"error","timestamp":1742596076295,"user_tz":-330,"elapsed":25029,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}},"outputId":"d28ab40a-bcf9-4127-e25b-10f89ef46882"},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11ef0bcf93b64dea80f3647eb39cf23d"}},"metadata":{}},{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 40081 has 14.69 GiB memory in use. Of the allocated memory 14.39 GiB is allocated by PyTorch, and 179.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-eb961ef11706>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcontrolnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfine_tuned_controlnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Remove torch_dtype parameter to use float32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m ).to(\"cuda\")\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Generate a floor plan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/pipelines/pipeline_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_loaded_in_4bit_bnb\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_loaded_in_8bit_bnb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/models/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1075\u001b[0m                     \u001b[0;34mf\"The current device is `{self.device}`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m                 )\n\u001b[0;32m-> 1077\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m     \u001b[0;31m# Taken from `transformers`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 40081 has 14.69 GiB memory in use. Of the allocated memory 14.39 GiB is allocated by PyTorch, and 179.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}]},{"cell_type":"code","source":["# Load the fine-tuned ControlNet model\n","fine_tuned_controlnet = ControlNetModel.from_pretrained(\"/content/drive/MyDrive/fine_tuned_controlnet\")\n","\n","# Load Stable Diffusion with the fine-tuned ControlNet\n","fine_tuned_pipe = StableDiffusionControlNetPipeline.from_pretrained(\n","    \"runwayml/stable-diffusion-v1-5\", controlnet=fine_tuned_controlnet, torch_dtype=torch.float16\n",").to(\"cuda\")\n","\n","# Generate a floor plan\n","prompt = \"a modern 3-room floor plan\"\n","control_image = Image.open(\"/content/drive/MyDrive/processed_dataset/3 rooms/Cat10_1.jpg\")  # Use a control image\n","output_image = fine_tuned_pipe(prompt, control_image, num_inference_steps=50).images[0]\n","\n","# Display the generated image\n","output_image.show()"],"metadata":{"id":"MucOCiEgO4il","executionInfo":{"status":"error","timestamp":1742595845122,"user_tz":-330,"elapsed":30520,"user":{"displayName":"Rana Talukdar 23bai10186","userId":"03414734647353131701"}},"colab":{"base_uri":"https://localhost:8080/","height":420,"referenced_widgets":["53f3b47986f2434cbe37a8f37cc6eb11","6650369f9a9d4fa9800fdf25fa25d6f4","945e6c53a92c4cb087f1226f8519ce47","829e45fa704547aa826ae0b8465cc8e1","d094badc961a4356b2dc93db9ae94e17","e76f45c0c94e49d7ac0c3cad70e94e26","83be80af52b94182b6e75dec6f48fdbe","bc5dc650224b43e2a0f55282547dc348","da1d83011add420b8645de8c55a70b46","f4861f1860354de7a1997e690107e66d","a6b367a09287464aadae3a47f037b74d","bfbe1e9e7f714d9f800425c19a59cfd9","6975f293622f4984adf877cd97d29289","fbbd87b99481475ba659c8e42a26fba4","8fdeb90ae47c4481981ed908aa3318ce","28a983dc8314407896b0e16f9ddb8619","6840cdbe39484eb481bb5197c15eb77f","9e163df3a1ed48e19bd9ceb91be477fa","410bc1b5ea634a9583e7a0b01afcbd03","cdf134e8334640d0a262f9f725a49e28","9bcdb0e37d12465783e9da7e669c0e00","60c50e4299fb45c0a232a6834f166913"]},"outputId":"ffedcc7a-2f30-44ae-bcf8-d9283c9588d3"},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53f3b47986f2434cbe37a8f37cc6eb11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/50 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfbe1e9e7f714d9f800425c19a59cfd9"}},"metadata":{}},{"output_type":"error","ename":"RuntimeError","evalue":"mat1 and mat2 must have the same dtype, but got Half and Float","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-07b30bd299ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"a modern 3-room floor plan\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcontrol_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/processed_dataset/3 rooms/Cat10_1.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use a control image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0moutput_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfine_tuned_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_inference_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Display the generated image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/pipelines/controlnet/pipeline_controlnet.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, prompt, image, height, width, num_inference_steps, timesteps, sigmas, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, controlnet_conditioning_scale, guess_mode, control_guidance_start, control_guidance_end, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1277\u001b[0m                     \u001b[0mcond_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrolnet_cond_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcontrolnet_keep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1279\u001b[0;31m                 down_block_res_samples, mid_block_res_sample = self.controlnet(\n\u001b[0m\u001b[1;32m   1280\u001b[0m                     \u001b[0mcontrol_model_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/models/controlnets/controlnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, controlnet_cond, conditioning_scale, class_labels, timestep_cond, attention_mask, added_cond_kwargs, cross_attention_kwargs, guess_mode, return_dict)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0mt_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestep_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m         \u001b[0maug_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sample, condition)\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcondition\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Half and Float"]}]},{"cell_type":"code","source":[],"metadata":{"id":"MXQSBnkMzIMr"},"execution_count":null,"outputs":[]}]}
